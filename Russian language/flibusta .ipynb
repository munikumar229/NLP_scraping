{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Data scrapping from flibusta.is wesite"
      ],
      "metadata": {
        "id": "PXAGcKiUxIFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rWSq6C8-CImA"
      },
      "outputs": [],
      "source": [
        "!pip install selenium -q\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "driver.get('https://flibusta.is/g/sf_fantasy')\n",
        "\n",
        "\n",
        "links = []\n",
        "\n",
        "for a in range(1, 9573):\n",
        "\n",
        "  url = driver.find_element(By.XPATH, f'//*[@id=\"main\"]/form/ol/a[{a}]').get_attribute('href')\n",
        "\n",
        "  driver1 = webdriver.Chrome(options=chrome_options)\n",
        "  driver1.get(url)\n",
        "\n",
        "\n",
        "\n",
        "  try:\n",
        "\n",
        "    for x in driver1.find_elements(By.XPATH, '//*[@id=\"main\"]/div[3]/a[2]'):\n",
        "        if x.text == \"(читать)\":\n",
        "            links.append(x.get_attribute('href'))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "link = []\n",
        "heading = []\n",
        "content = []\n",
        "\n",
        "\n",
        "for l in links:\n",
        "\n",
        "  link.append(l)\n",
        "  driver.get(l)\n",
        "  heading.append(driver.find_element(By.CLASS_NAME, 'title').text)\n",
        "\n",
        "  str = ''\n",
        "  for x in driver.find_elements(By.TAG_NAME, 'p'):\n",
        "    str += x.text\n",
        "\n",
        "  content.append(str)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'link':link,\n",
        "    'heading':heading,\n",
        "    'content':content\n",
        "})\n",
        "\n",
        "\n",
        "df.to_csv('flibusta.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.mkdir('input_folder')\n",
        "df = pd.read_csv('flibusta.csv')\n",
        "\n",
        "a = 0\n",
        "\n",
        "for x in df['content']:\n",
        "\n",
        "    try:\n",
        "        a +=1\n",
        "        with open(f'input_folder/{a}.txt', 'w') as f:\n",
        "             f.write(x)\n",
        "\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "ZTuU6xUjsrQ_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bad words removal"
      ],
      "metadata": {
        "id": "17KCmSswv_tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "bad_words = pd.read_csv('russian_badwords.csv')\n",
        "\n",
        "# Function to replace bad words with empty space in the text\n",
        "def replace_bad_words(text, bad_words):\n",
        "    pattern = '|'.join([re.escape(word) for word in bad_words])\n",
        "    return re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "# Function to process the files\n",
        "def process_txt_files(input_folder, output_folder, bad_words):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            input_file_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            cleaned_content = replace_bad_words(content, bad_words)\n",
        "            output_file_path = os.path.join(output_folder, filename)\n",
        "            with open(output_file_path, 'w', encoding='utf-8') as new_file:\n",
        "                new_file.write(cleaned_content)\n",
        "\n",
        "            print(f\"Processed file: {filename}\")\n",
        "\n",
        "input_folder = \"/content/input_folder\"\n",
        "output_folder = \"/content/cleaned_data\"\n",
        "process_txt_files(input_folder, output_folder, bad_words['0'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpYBDNe0ttl4",
        "outputId": "92d591b0-b8c5-44aa-bb6c-0f48c17c54ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file: 1.txt\n",
            "Processed file: 2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deduplication using Simhash"
      ],
      "metadata": {
        "id": "wVgt0HOHwdd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simhash -q\n",
        "from simhash import Simhash\n",
        "import os\n",
        "\n",
        "\n",
        "# function to calculate hamming disance b/w two hash values\n",
        "def hamming_distance(hash1, hash2):\n",
        "    x = hash1 ^ hash2\n",
        "    return bin(x).count('1')\n",
        "\n",
        "\n",
        "def calculate_similarity_percentage(hash1, hash2):\n",
        "    max_distance = 128\n",
        "    distance = hamming_distance(hash1, hash2)\n",
        "    similarity = ((max_distance - distance) / max_distance) * 100\n",
        "    return similarity\n",
        "\n",
        "\n",
        "# Function to check if a file is a duplicate based on its Simhash value\n",
        "def is_duplicate_file(file_hash, seen_hashes, similarity_threshold):\n",
        "    for seen_hash in seen_hashes:\n",
        "        similarity = calculate_similarity_percentage(file_hash, seen_hash)\n",
        "        if similarity >= similarity_threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Function to deduplicate files in a folder using Simhash\n",
        "def deduplicate_files_simhash(input_folder, output_folder, similarity_threshold=100):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    seen_hashes = []\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            input_file_path = os.path.join(input_folder, filename)\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "                file_content = file.read()\n",
        "\n",
        "            file_hash = Simhash(file_content, f=128).value\n",
        "\n",
        "            if not is_duplicate_file(file_hash, seen_hashes, similarity_threshold):\n",
        "                seen_hashes.append(file_hash)\n",
        "                output_file_path = os.path.join(output_folder, filename)\n",
        "                with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "                    output_file.write(file_content)\n",
        "                print(f\"Saved unique file: {filename}\")\n",
        "            else:\n",
        "                print(f\"Duplicate file skipped: {filename}\")\n",
        "\n",
        "\n",
        "input_folder = \"/content/cleaned_data\"\n",
        "output_folder = \"/content/simhash_deduplicated\"\n",
        "\n",
        "deduplicate_files_simhash(input_folder, output_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0egRdtietkSK",
        "outputId": "9198404f-9cf2-450d-c0a3-bed0e2a838e5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved unique file: 1.txt\n",
            "Saved unique file: 2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deduplication using Minhash\n"
      ],
      "metadata": {
        "id": "ShGtFj6zwlg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasketch\n",
        "from datasketch import MinHash\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# function to create a MinHash object for a set of tokens.\n",
        "def create_minhash(tokens, num_perm=128):\n",
        "    minhash = MinHash(num_perm=num_perm)\n",
        "    for token in tokens:\n",
        "        minhash.update(token.encode('utf8'))\n",
        "    return minhash\n",
        "\n",
        "# function to calculate jaccard similarity\n",
        "def calculate_jaccard_similarity(minhash1, minhash2):\n",
        "    return minhash1.jaccard(minhash2)\n",
        "\n",
        "# function for deduplication using Minhash\n",
        "def deduplicate_texts_minhash(texts, similarity_threshold=0.8, num_perm=128):\n",
        "    unique_texts = []\n",
        "    minhashes = []\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = tokenize(text)\n",
        "        minhash = create_minhash(tokens, num_perm)\n",
        "        is_duplicate = False\n",
        "        for existing_minhash in minhashes:\n",
        "            similarity = calculate_jaccard_similarity(minhash, existing_minhash)\n",
        "            if similarity >= similarity_threshold:\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if not is_duplicate:\n",
        "            minhashes.append(minhash)\n",
        "            unique_texts.append(text)\n",
        "\n",
        "    return unique_texts\n",
        "\n",
        "# function to process all text files\n",
        "def process_files_minhash(input_folder, output_folder, similarity_threshold=0.8, num_perm=128):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            input_file_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "                texts = file.readlines()\n",
        "            deduplicated_texts = deduplicate_texts_minhash(texts, similarity_threshold, num_perm)\n",
        "            output_file_path = os.path.join(output_folder, filename)\n",
        "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "                output_file.write('\\n'.join(deduplicated_texts))\n",
        "\n",
        "            print(f\"Processed file: {filename}\")\n",
        "\n",
        "input_folder = \"/content/cleaned_data\"\n",
        "output_folder = \"/content/simhash_deduplicated\"\n",
        "\n",
        "process_files_minhash(input_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "PaAKgWn_wqne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasketch import MinHash\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# function to create a MinHash object for a set of tokens.\n",
        "def create_minhash(tokens, num_perm=128):\n",
        "    minhash = MinHash(num_perm=num_perm)\n",
        "    for token in tokens:\n",
        "        minhash.update(token.encode('utf8'))\n",
        "    return minhash\n",
        "\n",
        "# function to calculate jaccard similarity\n",
        "def calculate_jaccard_similarity(minhash1, minhash2):\n",
        "    return minhash1.jaccard(minhash2)\n",
        "\n",
        "# Function to check if a file is a duplicate based on its MinHash value\n",
        "def is_duplicate_file(minhash, existing_minhashes, similarity_threshold):\n",
        "    for existing_minhash in existing_minhashes:\n",
        "        similarity = calculate_jaccard_similarity(minhash, existing_minhash)\n",
        "        if similarity >= similarity_threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to deduplicate files in a folder using MinHash\n",
        "def deduplicate_files_minhash(input_folder, output_folder, similarity_threshold=0.8, num_perm=128):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    existing_minhashes = []\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            input_file_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "                file_content = file.read()\n",
        "\n",
        "            tokens = tokenize(file_content)\n",
        "            file_minhash = create_minhash(tokens, num_perm)\n",
        "\n",
        "            if not is_duplicate_file(file_minhash, existing_minhashes, similarity_threshold):\n",
        "                existing_minhashes.append(file_minhash)\n",
        "                output_file_path = os.path.join(output_folder, filename)\n",
        "                with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "                    output_file.write(file_content)\n",
        "                print(f\"Saved unique file: {filename}\")\n",
        "            else:\n",
        "                print(f\"Duplicate file skipped: {filename}\")\n",
        "\n",
        "input_folder = \"/content/cleaned_data\"\n",
        "output_folder = \"/content/simhash_deduplicated\"\n",
        "\n",
        "deduplicate_files_minhash(input_folder, output_folder)"
      ],
      "metadata": {
        "id": "KQSPU5Eqwuoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}