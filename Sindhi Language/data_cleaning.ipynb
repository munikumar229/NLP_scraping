{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77d70d6-6cef-453b-a48f-84ebb6d080aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3bf1cd-11c1-44e8-b2a5-b3acae2a7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"dummy_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f451d88-8a61-4ee1-96ce-d3833d286c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwanted_words = [\" hi \", \" hello \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ed318e-13fc-42ba-8ba9-d581417366ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwanted_data_pattern = \"|\".join(unwanted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24019533-d568-4e55-80fa-ef48ea866946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_df = df[\"content\"].str.contains(unwanted_data_pattern, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04ba694-37b6-4c80-8f21-9b3abb7a5286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.bbc.com/news/articles/cd1646ewzleo</td>\n",
       "      <td>The mind-bending mirrors behind advanced techn...</td>\n",
       "      <td>High on a mountain, in Chile's bone dry Atacam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              url  \\\n",
       "0  https://www.bbc.com/news/articles/cd1646ewzleo   \n",
       "\n",
       "                                             heading  \\\n",
       "0  The mind-bending mirrors behind advanced techn...   \n",
       "\n",
       "                                             content  \n",
       "0  High on a mountain, in Chile's bone dry Atacam...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[~bool_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac6bb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimHash 1: 11001000011101110011000000001110000010000010010111011010000000000000100010000001110100100010100001011000010101011000001000100110\n",
      "SimHash 2: 11001100010100110011000000001110010010000010010101000100000000000000000010001010110000001011001101010000100011001001000000100110\n",
      "Hamming Distance: 28\n"
     ]
    }
   ],
   "source": [
    "# import hashlib\n",
    "# from collections import Counter\n",
    "\n",
    "# # Function to hash a token using MD5 and convert to binary\n",
    "# def hash_token(token):\n",
    "#     return bin(int(hashlib.md5(token.encode('utf-8')).hexdigest(), 16))[2:].zfill(128)\n",
    "\n",
    "# # Function to calculate SimHash\n",
    "# def simhash(text, weights=None):\n",
    "#     tokens = text.split()  # Simple whitespace tokenizer\n",
    "#     vector = [0] * 128  # 128-bit SimHash vector\n",
    "#     token_counts = Counter(tokens)\n",
    "    \n",
    "#     for token in tokens:\n",
    "#         token_hash = hash_token(token)\n",
    "#         for i in range(128):\n",
    "#             weight = weights[token] if weights else token_counts[token]\n",
    "#             if token_hash[i] == '1':\n",
    "#                 vector[i] += weight\n",
    "#             else:\n",
    "#                 vector[i] -= weight\n",
    "    \n",
    "#     # Generate final SimHash fingerprint\n",
    "#     simhash_value = ''.join(['1' if v > 0 else '0' for v in vector])\n",
    "#     return simhash_value\n",
    "\n",
    "# # Hamming distance function to compare two SimHash values\n",
    "# def hamming_distance(hash1, hash2):\n",
    "#     return sum(ch1 != ch2 for ch1, ch2 in zip(hash1, hash2))\n",
    "\n",
    "# # Example usage\n",
    "# text1 = \"نموني جو پهريون جملو\"  # Sindhi sentence\n",
    "# text2 = \"پهرين جملو نموني جو\"  # Similar sentence with reordering\n",
    "\n",
    "# hash1 = simhash(text1)\n",
    "# hash2 = simhash(text2)\n",
    "\n",
    "# distance = hamming_distance(hash1, hash2)\n",
    "# print(f\"SimHash 1: {hash1}\")\n",
    "# print(f\"SimHash 2: {hash2}\")\n",
    "# print(f\"Hamming Distance: {distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f45742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of neuralplot: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    matplotlib (>=3.1numpy>=1.16)\n",
      "               ~~~~~~^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simhash\n",
      "  Downloading simhash-2.1.2-py3-none-any.whl.metadata (382 bytes)\n",
      "Requirement already satisfied: numpy in c:\\users\\charanteja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simhash) (1.26.1)\n",
      "Downloading simhash-2.1.2-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: simhash\n",
      "Successfully installed simhash-2.1.2\n"
     ]
    }
   ],
   "source": [
    "# ! pip install simhash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b633c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Sindhi Texts:\n",
      "اهو هڪ مثال آهي\n",
      "پنجاب ۾ موسم بهتر آهي\n",
      "موسم بهتر آهي پنجاب ۾\n",
      "اهو ٻيو مثال آهي\n"
     ]
    }
   ],
   "source": [
    "# from simhash import Simhash\n",
    "# import pandas as pd\n",
    "\n",
    "# def deduplicate_text(texts):\n",
    "#     \"\"\"\n",
    "#     Remove duplicate texts using Simhash.\n",
    "\n",
    "#     :param texts: List of Sindhi text data\n",
    "#     :return: List of unique texts\n",
    "#     \"\"\"\n",
    "#     unique_texts = []\n",
    "#     seen_hashes = set()\n",
    "\n",
    "#     for text in texts:\n",
    "#         # Calculate the Simhash of the text\n",
    "#         text_hash = Simhash(text)\n",
    "        \n",
    "#         # Check if the hash is already seen\n",
    "#         if text_hash.value not in seen_hashes:\n",
    "#             seen_hashes.add(text_hash.value)\n",
    "#             unique_texts.append(text)\n",
    "\n",
    "#     return unique_texts\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Sample Sindhi text data\n",
    "#     sindhi_texts = [\n",
    "#         \"اهو هڪ مثال آهي\",\n",
    "#         \"اهو هڪ مثال آهي\",  # Duplicate\n",
    "#         \"پنجاب ۾ موسم بهتر آهي\",\n",
    "#         \"موسم بهتر آهي پنجاب ۾\",  # Similar but different structure\n",
    "#         \"اهو ٻيو مثال آهي\"\n",
    "#     ]\n",
    "\n",
    "#     unique_sindhi_texts = deduplicate_text(sindhi_texts)\n",
    "    \n",
    "#     # Display the final unique texts\n",
    "#     print(\"Unique Sindhi Texts:\")\n",
    "#     for text in unique_sindhi_texts:\n",
    "#         print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simhash import Simhash\n",
    "# import os\n",
    "\n",
    "# def deduplicate_text_file(input_file, output_file, threshold=3):\n",
    "#     # Set to store unique simhash values\n",
    "#     unique_hashes = set()\n",
    "#     # List to store final unique lines\n",
    "#     unique_lines = []\n",
    "\n",
    "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             # Compute the Simhash for the line\n",
    "#             line = line.strip()\n",
    "#             simhash_value = Simhash(line)\n",
    "\n",
    "#             # Check for duplicates\n",
    "#             if not any(abs(simhash_value.value - h) < threshold for h in unique_hashes):\n",
    "#                 unique_hashes.add(simhash_value.value)\n",
    "#                 unique_lines.append(line)\n",
    "\n",
    "#     # Write the unique lines to the output file\n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         for line in unique_lines:\n",
    "#             f.write(line + '\\n')\n",
    "\n",
    "# # Usage\n",
    "# input_file = 'D:\\IITGN\\NLP\\NLP_Scraping-main\\sindhi_tv_news\\2_article.txt'   # Replace with your input file name\n",
    "# output_file = 'D:\\IITGN\\NLP\\NLP_Scraping-main\\output.txt'  # Replace with your desired output file name\n",
    "# deduplicate_text_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4fda01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated content written to deduplicated_output.txt\n"
     ]
    }
   ],
   "source": [
    "# from simhash import Simhash\n",
    "# import os\n",
    "\n",
    "# def deduplicate_text_file(input_file):\n",
    "#     # Set to store unique simhash values\n",
    "#     unique_hashes = set()\n",
    "#     # List to store final unique lines\n",
    "#     unique_lines = []\n",
    "\n",
    "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             # Compute the Simhash for the line\n",
    "#             line = line.strip()\n",
    "#             simhash_value = Simhash(line)\n",
    "\n",
    "#             # Check for duplicates\n",
    "#             if not any(abs(simhash_value.value - h) < 3 for h in unique_hashes):\n",
    "#                 unique_hashes.add(simhash_value.value)\n",
    "#                 unique_lines.append(line)\n",
    "\n",
    "#     # Create an output file and write the unique lines\n",
    "#     output_file = 'deduplicated_output.txt'\n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         for line in unique_lines:\n",
    "#             f.write(line + '\\n')\n",
    "\n",
    "#     print(f'Deduplicated content written to {output_file}')\n",
    "\n",
    "# # Usage\n",
    "# input_file = r'D:\\IITGN\\NLP\\NLP_Scraping-main\\sindhi_tv_news\\2_article.txt'  # Replace with your input file name\n",
    "# deduplicate_text_file(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7f9c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simhash'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-53115309c5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimhash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeduplicate_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simhash'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from simhash import Simhash\n",
    "\n",
    "def deduplicate_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Remove duplicate texts from a text file using Simhash.\n",
    "\n",
    "    :param file_path: Path to the text file containing Sindhi text data\n",
    "    :return: List of unique texts\n",
    "    \"\"\"\n",
    "    unique_texts = []\n",
    "    seen_hashes = set()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = file.readlines()\n",
    "\n",
    "    for text in texts:\n",
    "        text = text.strip()  # Remove any leading/trailing whitespace\n",
    "        \n",
    "        # Calculate the Simhash of the text\n",
    "        text_hash = Simhash(text)\n",
    "        \n",
    "        # Check if the hash is already seen\n",
    "        if text_hash.value not in seen_hashes:\n",
    "            seen_hashes.add(text_hash.value)\n",
    "            unique_texts.append(text)\n",
    "\n",
    "    return unique_texts\n",
    "\n",
    "def deduplicate_nested_folders(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Deduplicate texts from all files in nested folders and write unique texts to new files,\n",
    "    preserving the folder structure.\n",
    "\n",
    "    :param input_folder: Path to the main input folder containing nested folders\n",
    "    :param output_folder: Path to the main output folder where unique text files will be saved\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            input_file_path = os.path.join(root, file)\n",
    "\n",
    "            try:\n",
    "                # Process the file to deduplicate texts\n",
    "                unique_texts = deduplicate_text_file(input_file_path)\n",
    "\n",
    "                # Determine the relative path and create the output directory\n",
    "                relative_path = os.path.relpath(root, input_folder)\n",
    "                output_subfolder = os.path.join(output_folder, relative_path)\n",
    "\n",
    "                if not os.path.exists(output_subfolder):\n",
    "                    os.makedirs(output_subfolder)\n",
    "\n",
    "                output_file_path = os.path.join(output_subfolder, file)\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write('\\n'.join(unique_texts))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {input_file_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "deduplicate_nested_folders('/mnt/HDFS1/language_nlp/russian_nlp_team_8/extracted_data', '/mnt/HDFS1/language_nlp/russian_nlp_team_8/unique_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d4861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from simhash import Simhash\n",
    "\n",
    "def deduplicate_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Remove duplicate texts from a text file using Simhash.\n",
    "\n",
    "    :param file_path: Path to the text file containing Sindhi text data\n",
    "    :return: List of unique texts\n",
    "    \"\"\"\n",
    "    unique_texts = []\n",
    "    seen_hashes = set()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts = file.readlines()\n",
    "\n",
    "    for text in texts:\n",
    "        text = text.strip()  # Remove any leading/trailing whitespace\n",
    "        \n",
    "        # Calculate the Simhash of the text\n",
    "        text_hash = Simhash(text)\n",
    "        \n",
    "        # Check if the hash is already seen\n",
    "        if text_hash.value not in seen_hashes:\n",
    "            seen_hashes.add(text_hash.value)\n",
    "            unique_texts.append(text)\n",
    "\n",
    "    return unique_texts\n",
    "\n",
    "def deduplicate_nested_folders(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Deduplicate texts from all text files in nested folders and write unique texts to new files,\n",
    "    preserving the folder structure.\n",
    "\n",
    "    :param input_folder: Path to the main input folder containing nested folders\n",
    "    :param output_folder: Path to the main output folder where unique text files will be saved\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                input_file_path = os.path.join(root, file)\n",
    "                unique_texts = deduplicate_text_file(input_file_path)\n",
    "\n",
    "                # Determine the relative path and create output directory\n",
    "                relative_path = os.path.relpath(root, input_folder)\n",
    "                output_subfolder = os.path.join(output_folder, relative_path)\n",
    "                \n",
    "                if not os.path.exists(output_subfolder):\n",
    "                    os.makedirs(output_subfolder)\n",
    "\n",
    "                output_file_path = os.path.join(output_subfolder, file)\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write('\\n'.join(unique_texts))\n",
    "\n",
    "# Example usage\n",
    "deduplicate_nested_folders(r'D:\\IITGN\\NLP\\NLP_Scraping-main\\learn_sindhi_lang_data', r'D:\\IITGN\\NLP\\NLP_Scraping-main\\cleaned_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b06516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
